{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844e35da-ed51-4b03-8a6d-4f21db3ce048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prade\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import logging\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab70044-c9ef-4839-9499-5dd73f81a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['pdf_pipeline']\n",
    "collection = db['pdf_metadata']\n",
    "\n",
    "# Function to read PDF content\n",
    "\n",
    "def extract_metadata(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        pdf_metadata = {\n",
    "            'name': os.path.basename(pdf_path),\n",
    "            'path': pdf_path,\n",
    "            'size': f'{(os.path.getsize(pdf_path))/(1024*1024)} Mb',\n",
    "            'num_pages': doc.page_count,\n",
    "            'pdf_type': 'Long PDFs' if doc.page_count > 30 else 'Medium PDFs' if 10 <= doc.page_count <= 30 else 'Short PDFs',\n",
    "            'status' : \"pending\"  \n",
    "        }\n",
    "        doc.close()\n",
    "        return pdf_metadata\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def ingest_pdfs(folder_path):\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    pdf_metadata_list = []\n",
    "\n",
    "    # Process PDFs in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for metadata in executor.map(extract_metadata, pdf_files):\n",
    "            if metadata:\n",
    "                # Insert the metadata into MongoDB\n",
    "                collection.insert_one(metadata)\n",
    "                pdf_metadata_list.append(metadata)\n",
    "    \n",
    "    return pdf_metadata_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5887b790-d36a-44d2-90f8-7ee28ca444ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_text(text, max_length=700, min_length=40):\n",
    "    # Prepare the text input for the model\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    # Generate summary with specified constraints\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=2, early_stopping=True)\n",
    "\n",
    "    # Decode the generated tokens into the final summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary.strip()\n",
    "\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    words = re.findall(r'\\w+', text.lower())  # Find words and convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words and word.isalpha()]# remove the stopword and numerical word\n",
    "    common_words = Counter(filtered_words).most_common(num_keywords)\n",
    "    keywords = [word for word, count in common_words]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5d6a88-b86f-48b7-ad68-c257202b48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_metadata):\n",
    "    try:\n",
    "        pdf_path=pdf_metadata['path']\n",
    "        doc = fitz.open(pdf_path)  # Open the PDF file\n",
    "        total_pages = doc.page_count\n",
    "        summary = \"\"\n",
    "        keywords=set()\n",
    "        chunk_size=250\n",
    "        # Process pages in chunks\n",
    "        for i in range(0, total_pages, chunk_size):\n",
    "            chunk_text = \"\"\n",
    "\n",
    "            # Loop through each page in the current chunk\n",
    "            for page_num in range(i, min(i + chunk_size, total_pages)):\n",
    "                page = doc.load_page(page_num)  # Load each page individually\n",
    "                chunk_text += page.get_text() \n",
    "            # Summarization and keyword extraction\n",
    "            summary += summarize_text(chunk_text) + ' '\n",
    "            keywords=keywords.union(set(extract_keywords(chunk_text)))\n",
    "        doc.close()\n",
    "        keywords=list(keywords)\n",
    "        # Update MongoDB with summary and keywords\n",
    "        collection.update_one(\n",
    "        {\"name\": os.path.basename(pdf_path)},\n",
    "        {\"$set\": {\"summary\": summary, \"keywords\": keywords, \"status\": \"processed\"}}\n",
    "    )\n",
    "        chunk_text=''\n",
    "        summary=''\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing PDF {pdf_metadata['path']}: {str(e)}\")\n",
    "\n",
    "def process_pdfs_in_parallel(pdf_metadata_list):\n",
    "    # Process PDFs concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(process_pdf, pdf_metadata_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6c40cf-9da5-4865-b5ee-d20a3ddef631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF processing completed.\n",
      " In 25.73214054107666 seconds\n"
     ]
    }
   ],
   "source": [
    "def main(folder_path):\n",
    "    start=time.time()\n",
    "    # Step 1: Ingest PDFs and store metadata in MongoDB\n",
    "    pdf_metadata_list = ingest_pdfs(folder_path)\n",
    "\n",
    "    # Step 2: Process each PDF (summarization and keyword extraction)\n",
    "    process_pdfs_in_parallel(pdf_metadata_list)\n",
    "    end=time.time()\n",
    "\n",
    "    print(f\"PDF processing completed.\\n In {end-start} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"C:/Users/Prade/OneDrive/Desktop/pdf\"  # Update this path\n",
    "    main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec754e9-f4ae-4dd2-baa7-46e814634a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all documents from the collection\n",
    "documents = collection.find()\n",
    "# Convert documents to a list of dictionaries\n",
    "documents_list = list(documents)\n",
    "\n",
    "# Convert to JSON format\n",
    "json_data = json.dumps(documents_list, default=str)  \n",
    "\n",
    "# store the JSON data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26792b40-4eba-4fc1-b41f-f09115a1571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pdf_info.json', 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32451091-512a-4c3e-b343-57951b0beb73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
